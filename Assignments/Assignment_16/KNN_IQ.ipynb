{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e8c94f6-7d27-4e60-abb6-084c47e39a89",
   "metadata": {},
   "source": [
    "# Interview Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638cc0aa-1c1b-49ce-8de4-a1c6be2d44f2",
   "metadata": {},
   "source": [
    "# 1. What are the key hyperparameters in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b377df-83be-4763-a9bb-10dcb2478423",
   "metadata": {},
   "source": [
    "The performance of the K-Nearest Neighbors (KNN) algorithm is influenced by the following key hyperparameters:\n",
    "\n",
    "1) n_neighbors (k): Specifies the number of nearest neighbors to consider when making a prediction.\n",
    "* Effect: A small value of k may lead to overfitting (sensitive to noise), while a large value of k may smooth the decision boundary but increase bias.\n",
    "\n",
    "2) weights: Determines the weight assigned to each neighbor’s contribution. Options include:\n",
    "   - \"uniform\": All neighbors contribute equally.\n",
    "   - \"distance\": Closer neighbors have a higher influence.\n",
    "* Effect: The choice affects how much influence distant neighbors have on the prediction.\n",
    "\n",
    "3) metric: Specifies the distance metric used to calculate the closeness of neighbors (e.g., Euclidean, Manhattan).\n",
    "* Effect: The distance metric influences how \"similarity\" is defined, potentially affecting accuracy depending on the data's structure.\n",
    "\n",
    "4) p (for Minkowski distance): Determines the power parameter for the Minkowski distance.\n",
    "* Effect: If p=1, it corresponds to Manhattan distance; if p=2, it corresponds to Euclidean distance.\n",
    "\n",
    "5) algorithm: Determines the method used to compute nearest neighbors (e.g., \"auto\", \"ball_tree\", \"kd_tree\", or \"brute\").\n",
    "* Effect: Impacts computational efficiency, particularly for large datasets.\n",
    "\n",
    "6) leaf_size (for tree-based algorithms): Specifies the size of the leaf in kd_tree or ball_tree.\n",
    "* Effect: Impacts the speed of queries and memory usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd15ae0-0dd0-4a74-ad98-9edd197988b6",
   "metadata": {},
   "source": [
    "# 2. What distance metrics can be used in KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1755c5df-213c-4aab-9151-6c42be920eea",
   "metadata": {},
   "source": [
    "In the K-Nearest Neighbors (KNN) algorithm, the choice of distance metric determines how similarity or closeness between data points is measured. Here are the common distance metrics used:\n",
    "\n",
    "1) Euclidean Distance:\n",
    "* A popular metric for continuous data, it measures the straight-line distance between two points in multi-dimensional space. It works well when the dataset has numeric features that are scaled appropriately.\n",
    "\n",
    "2) Manhattan Distance:\n",
    "* This measures the distance between points along the grid (sum of absolute differences). It is robust to outliers and is suitable when the features are independent and not highly correlated.\n",
    "\n",
    "3) Minkowski Distance:\n",
    "* A generalization of Euclidean and Manhattan distances, this metric provides flexibility by allowing the user to adjust how distances are calculated depending on the data structure and sensitivity to outliers.\n",
    "\n",
    "4) Hamming Distance:\n",
    "* Used for categorical or binary data, this metric calculates the number of differing attributes between two data points. It’s ideal for datasets where features are non-numeric, such as text or binary flags.\n",
    "\n",
    "5) Cosine Distance:\n",
    "* Commonly used for high-dimensional data such as text or document similarity, it measures the angle between two vectors rather than their magnitude. It’s useful when the magnitude of the data is less important than the direction.\n",
    "\n",
    "6) Mahalanobis Distance:\n",
    "* This metric considers correlations between features and scales appropriately for datasets with varying feature distributions. It’s particularly useful when features have different units or when multivariate relationships exist.\n",
    "\n",
    "The choice of metric depends on the type of data and the problem. For instance, Euclidean distance works well for numerical data, Hamming distance is suited for categorical data, and Cosine distance is effective in text or sparse data scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
